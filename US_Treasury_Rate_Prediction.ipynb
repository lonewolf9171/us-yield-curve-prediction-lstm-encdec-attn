{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "US Treasury Rate Prediction.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# US Treasury Yield Prediction\n",
        "\n",
        "Reference: https://ieeexplore.ieee.org/document/9049511 <br>\n",
        "Data: https://home.treasury.gov/ <br>\n",
        "Domain: Multi Dimensional Multistep Forecasting<br><br>\n",
        "We use Deep Neural Networks to predict the yield curve for next 10 days based on the observations of last 30 days. Specifically we use an Long Short Term Memory (LSTM) based Encoder Decoder network architecture with Attention mechanism."
      ],
      "metadata": {
        "id": "bVnowRJnCBm2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irBYRRhG9OHU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch.nn.modules import dropout"
      ],
      "metadata": {
        "id": "i60o2Q84-Yj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use(\"Solarize_Light2\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 7)\n",
        "# plt.figure(dpi=300)"
      ],
      "metadata": {
        "id": "68ErvhE27oAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uIeLW-8iU8UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Projects/Deep Bonds/data/\""
      ],
      "metadata": {
        "id": "2pybtNPf7cnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yield_df = pd.read_csv(data_path + \"US_Treasury_Yield.csv\")\n",
        "yield_df[\"Date\"] = yield_df[\"Date\"].apply(lambda  x: datetime.datetime.strptime(str(x), \"%m/%d/%y\"))\n",
        "yield_df = yield_df.sort_values([\"Date\"])\n",
        "yield_df.drop([\"2 Mo\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "0_x1svuY9aUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yield_df.iloc[5000:5005]"
      ],
      "metadata": {
        "id": "zBTWuMFgQO5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yield_df.describe()"
      ],
      "metadata": {
        "id": "4bgy8dFLw-6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df = yield_df.copy()\n",
        "main_df = main_df.sort_values([\"Date\"])"
      ],
      "metadata": {
        "id": "W3McZlZ6FQ5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Data is handled with interpolation in the Yield Curve. That is the missing values are replaced by the adjacent term rate."
      ],
      "metadata": {
        "id": "UGvvZi2NE7Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main_df.iloc[:, 1:] = main_df.iloc[:, 1:].interpolate(axis=1, limit_direction=\"both\")\n",
        "for c in list(main_df.columns)[1:]:\n",
        "  main_df.loc[main_df[c].isna(), c] = main_df.loc[main_df[c].isna(), c].fillna(main_df[c].mean())\n",
        "# main_df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "ouFZccyy9ajG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridx = np.random.randint(0, len(yield_df), 1)\n",
        "main_df.iloc[ridx]"
      ],
      "metadata": {
        "id": "Gk9AuyBco4v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(np.array(yield_df.iloc[ridx].drop([\"Date\"], axis=1)).squeeze(), \">\")\n",
        "plt.xticks(list(range(12)), list(main_df.columns)[1:])\n",
        "plt.title(\"Yield Curve for: {}\".format(list(main_df.iloc[ridx][\"Date\"])[0].date()))\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x2YQAcPuozZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ridx = np.random.randint(0, len(yield_df), 1)\n",
        "plt.plot(np.array(main_df.iloc[ridx].drop([\"Date\"], axis=1)).squeeze(), \"r<\", label=\"Interpolated\")\n",
        "plt.plot(np.array(yield_df.iloc[ridx].drop([\"Date\"], axis=1)).squeeze(), \">\", label=\"Actual\")\n",
        "plt.xticks(list(range(12)), list(main_df.columns)[1:])\n",
        "plt.title(\"Yield Curve for: {}\".format(list(main_df.iloc[ridx][\"Date\"])[0].date()))\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ohyfZyHa6XYy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
        "for c in list(yield_df.columns)[1:]:\n",
        "  plt.plot(list(yield_df[\"Date\"]), yield_df[c], \"-\", alpha=0.8, label=c)\n",
        "plt.legend()\n",
        "plt.title(\"Evolution of Term Structure\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dMTFwoC66qwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the data for traning using a Minmax Scaler which assigns a value of 1 to the maximum value and 0 to the minimum."
      ],
      "metadata": {
        "id": "S8QMt26GFUJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "main_df.iloc[:, 1:] = scaler.fit_transform(main_df.iloc[:, 1:])"
      ],
      "metadata": {
        "id": "xRtmhRAZ59AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_df.head()"
      ],
      "metadata": {
        "id": "fWS6my8b8uf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "look_back = 30\n",
        "look_ahead = 10"
      ],
      "metadata": {
        "id": "sPWNSQCrL0vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(look_back, len(main_df)):\n",
        "  if i + look_ahead < len(main_df):\n",
        "    _x, _y = main_df.iloc[i-look_back: i], main_df.iloc[i: i + look_ahead]\n",
        "    _x, _y = np.array(_x.drop([\"Date\"], axis=1)), np.array(_y.drop([\"Date\"], axis=1))\n",
        "    X_data.append(_x), y_data.append(_y)\n"
      ],
      "metadata": {
        "id": "w1Tcl5WK9alP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = torch.from_numpy(np.array(X_data)).float()\n",
        "y_tensor = torch.from_numpy(np.array(y_data)).float()"
      ],
      "metadata": {
        "id": "4riZpOQL9anS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dl = y_tensor.shape[0]\n",
        "batch_size = 32\n",
        "\n",
        "train, valid, test = random_split(dataset, [int(0.75*dl), int(0.15*dl), dl - int(0.75*dl) - int(0.15*dl)])\n",
        "train_dl = DataLoader(train, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "valid_dl = DataLoader(valid, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_dl = DataLoader(test, shuffle=False, batch_size=batch_size, drop_last=True)"
      ],
      "metadata": {
        "id": "VK-YJYEfP8o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderModel(nn.Module):\n",
        "  def __init__(self, input_dim, batch_size, hidden_dim, n_layers):\n",
        "    super(EncoderModel, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_dim, \n",
        "                        batch_first=True, num_layers=self.n_layers)\n",
        "    \n",
        "  def forward(self, input_seq, hidden):\n",
        "    output, hidden = self.lstm(input_seq, hidden)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "Bqsg1WuKP8li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnEncoderModel(nn.Module):\n",
        "  def __init__(self, input_dim, batch_size, hidden_dim, n_layers, input_seq=look_back, output_seq=look_ahead):\n",
        "    super(AttnEncoderModel, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.input_seq = input_seq\n",
        "    self.output_seq = output_seq\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_dim, \n",
        "                        batch_first=True, num_layers=self.n_layers, dropout=0.2)\n",
        "    \n",
        "    self.attn = nn.Linear(self.hidden_dim, 1)\n",
        "    \n",
        "  def forward(self, input_seq, hidden):\n",
        "    output, hidden = self.lstm(input_seq, hidden)\n",
        "    o1 = self.attn(output).squeeze(-1)\n",
        "    weights = nn.Softmax(dim=1)(o1)\n",
        "    weights = weights.repeat(1, 1, self.hidden_dim).view(self.batch_size, self.hidden_dim, self.input_seq)\n",
        "    attn_mul = weights.transpose(2, 1) * output\n",
        "    attn_mul = attn_mul.sum(dim=1)\n",
        "    output = attn_mul.repeat(1, self.output_seq).reshape(-1, self.output_seq, self.hidden_dim)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "JnwxufZRykjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderModel(nn.Module):\n",
        "  def __init__(self, input_dim, batch_size, hidden_dim, n_layers):\n",
        "    super(DecoderModel, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_dim, \n",
        "                        batch_first=True, num_layers=self.n_layers, dropout=0.2)\n",
        "    \n",
        "  def forward(self, enco, hidden):\n",
        "    output, hidden = self.lstm(enco, hidden)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "UhL4PFWGP8ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncDecModel(nn.Module):\n",
        "  def __init__(self, encoder, decoder, output_seq, output_dim):\n",
        "    super(EncDecModel, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.output_seq = output_seq\n",
        "\n",
        "  def forward(self, input_seq, hidden):\n",
        "    out1, hidden = self.encoder(input_seq, hidden)\n",
        "    out1 = out1[:, -self.output_seq:, :]\n",
        "    out1 = nn.ReLU()(out1)\n",
        "    out2, hidden = self.decoder(out1, hidden)\n",
        "    out2 = out2[:, :, -self.output_dim:]\n",
        "    # out2 = nn.ReLU()(out2)\n",
        "    return out2, hidden"
      ],
      "metadata": {
        "id": "mZ8jEFSRP8fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 11\n",
        "input_seq = look_back\n",
        "\n",
        "output_seq = look_ahead\n",
        "output_dim = input_dim\n",
        "\n",
        "enc_hidden = 300\n",
        "dec_hidden = enc_hidden\n",
        "\n",
        "n_layers = 4\n",
        "\n",
        "# enc_model = EncoderModel(input_dim=input_dim, hidden_dim=enc_hidden, \n",
        "#                         n_layers=n_layers, batch_size=batch_size)\n",
        "\n",
        "enc_model = AttnEncoderModel(input_dim=input_dim, hidden_dim=enc_hidden, \n",
        "                        n_layers=n_layers, batch_size=batch_size, \n",
        "                        input_seq=input_seq, output_seq=output_seq)\n",
        "\n",
        "dec_model = DecoderModel(input_dim=enc_hidden, hidden_dim=dec_hidden,\n",
        "                        n_layers=n_layers, batch_size=batch_size)\n",
        "\n",
        "model = EncDecModel(encoder=enc_model, decoder=dec_model, \n",
        "                            output_seq=output_seq, output_dim=output_dim)"
      ],
      "metadata": {
        "id": "IrWOASpEP8cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.HuberLoss()\n",
        "n_epochs = 150"
      ],
      "metadata": {
        "id": "YiGya6C1qEm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patience=3\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  # if patience == 0:\n",
        "  #     print(\"Early Stopping the training.\\nEpoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\").format(epoch, train_loss, valid_loss)\n",
        "  #     break\n",
        "\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0  \n",
        "  \n",
        "  hidden = (torch.zeros(n_layers, batch_size, enc_hidden).requires_grad_().cuda(),\n",
        "            torch.zeros(n_layers, batch_size, enc_hidden).requires_grad_().cuda())\n",
        "\n",
        "  model.train()\n",
        "  for xt, yt in train_dl:\n",
        "    xt, yt = xt.cuda(), yt.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    hidden = tuple([each.data for each in hidden])\n",
        "    out, hidden = model(xt, hidden)\n",
        "    loss = criterion(out, yt)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item() * xt.size(0)\n",
        "  \n",
        "  hidden = (torch.zeros(n_layers, batch_size, enc_hidden).cuda(),\n",
        "            torch.zeros(n_layers, batch_size, enc_hidden).cuda())\n",
        "\n",
        "  model.eval()\n",
        "  for xv, yv in valid_dl:\n",
        "    xv, yv = xv.cuda(), yv.cuda()\n",
        "    with torch.no_grad():\n",
        "      outv, _ = model(xv, hidden)\n",
        "      lossv = criterion(outv, yv)\n",
        "      valid_loss += lossv.item() * xv.size(0)\n",
        "  \n",
        "  train_loss = train_loss/len(train_dl.sampler)\n",
        "  valid_loss = valid_loss/len(valid_dl.sampler)\n",
        "\n",
        "  if epoch%5 == 0:\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
        "  \n",
        "  train_losses.append(train_loss)\n",
        "  valid_losses.append(valid_loss)\n",
        "\n",
        "  if epoch > 5 and valid_loss > valid_losses[-5]:\n",
        "    patience -= 1\n"
      ],
      "metadata": {
        "id": "qTwBAl7HqEsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(train_losses, \"r\", label=\"Train Error\")\n",
        "plt.plot(valid_losses, label=\"Validation Error\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss Comparision\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_KqFiMBGqEu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outs = []\n",
        "truey = []\n",
        "\n",
        "hidden = (torch.zeros(n_layers, batch_size, enc_hidden).requires_grad_().cuda(),\n",
        "            torch.zeros(n_layers, batch_size, enc_hidden).requires_grad_().cuda())\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for xte, yte in test_dl:\n",
        "    xte, yte = xte.cuda(), yte.cuda()\n",
        "    outte, _ = model(xte, hidden)\n",
        "    outs.append(outte)\n",
        "    truey.append(yte)"
      ],
      "metadata": {
        "id": "3s9CwfYvqExO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "otte = torch.concat(outs)\n",
        "trye = torch.concat(truey)"
      ],
      "metadata": {
        "id": "I2NqVgNUqEzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "  random_idx = np.random.randint(0, len(otte), 1)[0]\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(15, 4), sharey=True)\n",
        "  plt.figure(dpi=50)\n",
        "  plt.setp(ax, xticks=list(range(12)),xticklabels = list(main_df.columns)[1:])\n",
        "  for i in range(look_ahead):\n",
        "    r = scaler.inverse_transform(otte[random_idx, i, :].cpu().numpy().reshape(1, -1)).squeeze()\n",
        "    ax[1].plot(r, \"-\",label=f\"Day t+{i+1}\")\n",
        "  ax[1].legend()\n",
        "  ax[1].set_title(f\"Predicted Yield Curve next {look_ahead} days\")\n",
        "  # print(\"\\n\")\n",
        "\n",
        "  for i in range(look_ahead):\n",
        "    r = scaler.inverse_transform(trye[random_idx, i, :].cpu().numpy().reshape(1, -1)).squeeze()\n",
        "    ax[0].plot(r, \"-\",label=f\"Day t+{i+1}\")\n",
        "  ax[0].legend()\n",
        "  ax[0].set_title(f\"True Yield Curve next {look_ahead} days\")\n",
        "  plt.show()\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "mJ9RMUSQFsCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = torch.from_numpy(scaler.inverse_transform(otte.view(-1, 11).cpu().numpy())).float().view(-1, look_ahead, 11)\n",
        "y_trues = torch.from_numpy(scaler.inverse_transform(trye.view(-1, 11).cpu().numpy())).float().view(-1, look_ahead, 11)\n",
        "# y_preds = scaler.inverse_transform(otte.view(-1, 11).cpu().numpy()).reshape(-1, look_ahead, 11)\n",
        "# y_trues = scaler.inverse_transform(trye.view(-1, 11).cpu().numpy()).reshape(-1, look_ahead, 11)\n",
        "\n",
        "print(\"\\nAverage Absoulte Error for Day\")\n",
        "for i in range(look_ahead):\n",
        "  l = nn.L1Loss()(y_preds[:, i, :], y_trues[:, i, :]).item()\n",
        "  # l = np.abs(y_trues[:, i, :] - y_preds[:, i, :])/(y_trues[:, i, :])\n",
        "  # l = np.round(l.reshape(-1), 5).mean(dtype=np.float64)\n",
        "  # l = l.mean(dtype=np.float64)\n",
        "  print(f\"Day t+{i+1} :\", l)"
      ],
      "metadata": {
        "id": "-s01laCWL6Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average Absoulte Error for Term:\")\n",
        "for i in range(11):\n",
        "  l = nn.L1Loss()(y_preds[:, :, i], y_trues[:, :, i]).item()\n",
        "  print(list(main_df.columns)[1:][i], \":\",l)"
      ],
      "metadata": {
        "id": "Wzu-7C_dndPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}